{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2eabff-d2bf-43dc-bae8-cef30a0daac1",
   "metadata": {},
   "source": [
    "***Задание*** А: к этапу предобработки текстов добавить фильтр по частям речи. То есть после предобратки в текстах должны оставаться только слова определенных частей речи. Попробовать: 1) только сущ., 2) сущ.+прил., 3) сущ.+прил.+гл. 4) свой вариант. Этот эта предобработки соединить со спобами представления текста через вектор:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007abff-5ae6-4f25-abdb-42b29fa95fd9",
   "metadata": {},
   "source": [
    "1) мешок слов,2) tf-idf, 3) LSI 4) LDA\n",
    "2) Использовать 2 алгорится классификации, которые лучше всегосебя показали в предыдущих исследованиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685805f7-7d1d-467b-86d4-4f74b6cb0886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/igorkopylov/PycharmProjects/liner_model/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim import corpora, models\n",
    "from nltk import pos_tag\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e556b-85b7-4523-84f3-56c36c2e4d2e",
   "metadata": {},
   "source": [
    "# Формирование текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbef744-006c-4933-adb2-f03e519b01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего элементов: 2245\n",
      "ко-во элементов из класса 0: 480\n",
      "ко-во элементов из класса 1: 584\n",
      "ко-во элементов из класса 2: 591\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "number_of_topic = 3\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "indices = np.where((newsgroups_train.target >= 0) & (newsgroups_train.target <= number_of_topic))[0]\n",
    "texts = [newsgroups_train.data[i] for i in indices]\n",
    "target = np.array([newsgroups_train.target[i] for i in indices])\n",
    "print(f\"Всего элементов: {target.shape[0]}\")\n",
    "for i in range(number_of_topic):\n",
    "    count = np.count_nonzero(target == i)\n",
    "    print(f\"ко-во элементов из класса {i}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d1267d-e7f6-4c82-a08a-e042b80fb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize_tokens(tokens):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Лемматизация слова \"running\"\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "\n",
    "def preprocess_text(text): \n",
    "    # Удаление пунктуации\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)  #spacy_lemmatize_tokens(tokens)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75dbc3a4-5544-4592-a1fb-582b48082bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From', 'jgreenamber', 'Joe', 'Green', 'Subject']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [preprocess_text(i) for i in texts]\n",
    "tokenized_corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ab17fd-dcc7-4316-ae62-b0c0b183b502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 60] Operation timed out>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_part_speech(token_list, entities):\n",
    "    # 1) только сущ., 2) сущ.+прил., 3) сущ.+прил.+гл. 4) \n",
    "    # N - noun; V - verb; JJ - adjectives\n",
    "    pos_tags = pos_tag(token_list, lang='eng')\n",
    "    part_speech = []\n",
    "    for word, pos in pos_tags:\n",
    "        if any(pos.startswith(j) for j in entities):\n",
    "           part_speech.append(word)\n",
    "    return part_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a315c0b-760e-4a81-af54-d3020b3fba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus_n = [get_part_speech(i, ['N']) for i in tokenized_corpus]\n",
    "tokenized_corpus_na = [get_part_speech(i, ['N', 'JJ']) for i in tokenized_corpus]\n",
    "tokenized_corpus_nav = [get_part_speech(i, ['N', 'JJ', 'V']) for i in tokenized_corpus]\n",
    "tokenized_corpus_nin = [get_part_speech(i, ['N', 'IN']) for i in tokenized_corpus]\n",
    "# 'IN' - предлоги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada30cc9-6e75-4943-8c2e-91c8dd078e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpuses = [tokenized_corpus_n, tokenized_corpus_na, tokenized_corpus_nav, tokenized_corpus_nin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3ef8e6-0e6e-4ed4-b214-44c8802d48b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jgreenamber',\n",
       " 'Joe',\n",
       " 'Green',\n",
       " 'Subject',\n",
       " 'Re',\n",
       " 'Weitek',\n",
       " 'P9000',\n",
       " 'Organization',\n",
       " 'Harris',\n",
       " 'Computer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_n[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8eac2d8-e700-49e9-8c93-ae4b682274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_corpuses = []\n",
    "for corp in tokenized_corpuses:\n",
    "    documents = []\n",
    "    for doc in corp:\n",
    "        documents.append(' '.join(doc))\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tf_idf_corpuses.append(tfidf_vectorizer.fit_transform(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7302fda8-e085-49ba-9269-194aefd631cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words_corpuses = []\n",
    "for corp in tokenized_corpuses:\n",
    "    documents = []\n",
    "    for doc in corp:\n",
    "        documents.append(' '.join(doc))\n",
    "    vectorizer = CountVectorizer()\n",
    "    count_words_corpuses.append(vectorizer.fit_transform(documents).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0045682c-daa5-4156-a848-f1882a46175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_corpuses = []\n",
    "for corp in count_words_corpuses:\n",
    "    documents = []\n",
    "    for doc in corp:\n",
    "        total_words_per_document = np.sum(doc)\n",
    "        word_frequency_matrix = doc / total_words_per_document\n",
    "        documents.append(word_frequency_matrix)\n",
    "    freq_words_corpuses.append(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88189310-aee7-4b77-ab41-6ff7fa793d82",
   "metadata": {},
   "source": [
    "# Cкармливание документов моделям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac4963-fbee-4f12-811b-47243119822d",
   "metadata": {},
   "source": [
    "lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "166d4e6a-a229-4c37-8a53-b57c0d8b7644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Модель с alpha=symmetric, eta=auto:\n",
      "Топик: 0 \n",
      "Слова: 0.006*\"writes\" + 0.006*\"Re\" + 0.006*\"God\" + 0.005*\"Subject\" + 0.005*\"people\" + 0.005*\"Lines\" + 0.005*\"Organization\" + 0.004*\"article\" + 0.004*\"dont\" + 0.004*\"think\"\n",
      "Топик: 1 \n",
      "Слова: 0.010*\"Subject\" + 0.010*\"Lines\" + 0.009*\"Organization\" + 0.008*\"drive\" + 0.005*\"file\" + 0.005*\"University\" + 0.005*\"card\" + 0.004*\"Windows\" + 0.004*\"use\" + 0.004*\"Re\"\n",
      "Топик: 2 \n",
      "Слова: 0.063*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.011*\"M\" + 0.009*\"P\" + 0.002*\"D\" + 0.001*\"px\" + 0.001*\"MG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9V\" + 0.001*\"MP\" + 0.001*\"H\" + 0.001*\"MO\" + 0.001*\"G\"\n",
      "\n",
      "Модель с alpha=symmetric, eta=0.1:\n",
      "Топик: 0 \n",
      "Слова: 0.016*\"drive\" + 0.007*\"Subject\" + 0.007*\"writes\" + 0.006*\"article\" + 0.006*\"Lines\" + 0.006*\"Re\" + 0.006*\"system\" + 0.006*\"Organization\" + 0.005*\"dont\" + 0.005*\"know\"\n",
      "Топик: 1 \n",
      "Слова: 0.014*\"Subject\" + 0.014*\"Lines\" + 0.014*\"Organization\" + 0.009*\"file\" + 0.008*\"University\" + 0.007*\"Windows\" + 0.006*\"card\" + 0.006*\"driver\" + 0.006*\"program\" + 0.006*\"use\"\n",
      "Топик: 2 \n",
      "Слова: 0.063*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.017*\"God\" + 0.011*\"atheist\" + 0.008*\"people\" + 0.008*\"M\" + 0.008*\"Re\" + 0.007*\"think\" + 0.007*\"religion\" + 0.007*\"writes\" + 0.007*\"god\"\n",
      "\n",
      "Модель с alpha=symmetric, eta=0.01:\n",
      "Топик: 0 \n",
      "Слова: 0.023*\"drive\" + 0.010*\"system\" + 0.009*\"writes\" + 0.008*\"article\" + 0.008*\"dont\" + 0.007*\"Re\" + 0.007*\"Subject\" + 0.007*\"time\" + 0.007*\"disk\" + 0.007*\"Lines\"\n",
      "Топик: 1 \n",
      "Слова: 0.022*\"Subject\" + 0.021*\"Lines\" + 0.021*\"Organization\" + 0.013*\"file\" + 0.012*\"University\" + 0.011*\"Windows\" + 0.010*\"card\" + 0.009*\"driver\" + 0.009*\"program\" + 0.008*\"know\"\n",
      "Топик: 2 \n",
      "Слова: 0.076*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.020*\"God\" + 0.013*\"atheist\" + 0.012*\"people\" + 0.012*\"Re\" + 0.011*\"say\" + 0.010*\"writes\" + 0.010*\"think\" + 0.010*\"Subject\" + 0.009*\"M\"\n",
      "\n",
      "Модель с alpha=asymmetric, eta=auto:\n",
      "Топик: 0 \n",
      "Слова: 0.008*\"Subject\" + 0.008*\"drive\" + 0.008*\"Lines\" + 0.008*\"Organization\" + 0.006*\"Re\" + 0.006*\"writes\" + 0.005*\"card\" + 0.005*\"article\" + 0.005*\"system\" + 0.005*\"dont\"\n",
      "Топик: 1 \n",
      "Слова: 0.010*\"file\" + 0.009*\"Subject\" + 0.009*\"Lines\" + 0.009*\"Organization\" + 0.006*\"_\" + 0.006*\"Windows\" + 0.005*\"program\" + 0.005*\"image\" + 0.005*\"University\" + 0.004*\"use\"\n",
      "Топик: 2 \n",
      "Слова: 0.045*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.009*\"M\" + 0.006*\"P\" + 0.004*\"Islam\" + 0.004*\"atheism\" + 0.003*\"atheist\" + 0.003*\"moral\" + 0.002*\"religion\" + 0.002*\"morality\" + 0.002*\"belief\"\n",
      "\n",
      "Модель с alpha=asymmetric, eta=0.1:\n",
      "Топик: 0 \n",
      "Слова: 0.012*\"Subject\" + 0.012*\"Lines\" + 0.011*\"Organization\" + 0.010*\"drive\" + 0.007*\"Re\" + 0.006*\"know\" + 0.006*\"card\" + 0.006*\"writes\" + 0.006*\"article\" + 0.005*\"get\"\n",
      "Топик: 1 \n",
      "Слова: 0.019*\"file\" + 0.012*\"_\" + 0.011*\"M\" + 0.010*\"image\" + 0.009*\"Subject\" + 0.009*\"Lines\" + 0.009*\"P\" + 0.009*\"Organization\" + 0.008*\"program\" + 0.007*\"point\"\n",
      "Топик: 2 \n",
      "Слова: 0.062*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.016*\"God\" + 0.011*\"people\" + 0.011*\"atheist\" + 0.008*\"say\" + 0.008*\"think\" + 0.007*\"writes\" + 0.007*\"religion\" + 0.007*\"believe\" + 0.007*\"god\"\n",
      "\n",
      "Модель с alpha=asymmetric, eta=0.01:\n",
      "Топик: 0 \n",
      "Слова: 0.017*\"drive\" + 0.013*\"Subject\" + 0.012*\"Lines\" + 0.012*\"Organization\" + 0.009*\"card\" + 0.009*\"system\" + 0.009*\"Re\" + 0.009*\"writes\" + 0.008*\"article\" + 0.008*\"get\"\n",
      "Топик: 1 \n",
      "Слова: 0.021*\"Subject\" + 0.020*\"Lines\" + 0.020*\"Organization\" + 0.020*\"file\" + 0.015*\"Windows\" + 0.012*\"program\" + 0.012*\"_\" + 0.012*\"University\" + 0.011*\"M\" + 0.010*\"image\"\n",
      "Топик: 2 \n",
      "Слова: 0.076*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.020*\"God\" + 0.014*\"people\" + 0.013*\"atheist\" + 0.011*\"say\" + 0.010*\"Re\" + 0.010*\"think\" + 0.010*\"writes\" + 0.009*\"believe\" + 0.008*\"religion\"\n",
      "\n",
      "Модель с alpha=0.1, eta=auto:\n",
      "Топик: 0 \n",
      "Слова: 0.006*\"God\" + 0.006*\"Re\" + 0.006*\"writes\" + 0.006*\"people\" + 0.006*\"Subject\" + 0.005*\"Lines\" + 0.005*\"Organization\" + 0.005*\"think\" + 0.004*\"dont\" + 0.004*\"article\"\n",
      "Топик: 1 \n",
      "Слова: 0.009*\"Subject\" + 0.009*\"Lines\" + 0.009*\"Organization\" + 0.008*\"drive\" + 0.005*\"file\" + 0.005*\"University\" + 0.005*\"card\" + 0.004*\"Windows\" + 0.004*\"use\" + 0.004*\"Re\"\n",
      "Топик: 2 \n",
      "Слова: 0.063*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.011*\"M\" + 0.009*\"P\" + 0.002*\"D\" + 0.001*\"px\" + 0.001*\"MG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9VG9V\" + 0.001*\"MP\" + 0.001*\"H\" + 0.001*\"MO\" + 0.001*\"G\"\n",
      "\n",
      "Модель с alpha=0.1, eta=0.1:\n",
      "Топик: 0 \n",
      "Слова: 0.008*\"writes\" + 0.007*\"Re\" + 0.007*\"people\" + 0.007*\"article\" + 0.007*\"dont\" + 0.007*\"God\" + 0.007*\"Subject\" + 0.006*\"Lines\" + 0.006*\"say\" + 0.006*\"Organization\"\n",
      "Топик: 1 \n",
      "Слова: 0.013*\"Subject\" + 0.013*\"Lines\" + 0.013*\"Organization\" + 0.008*\"drive\" + 0.008*\"file\" + 0.007*\"University\" + 0.007*\"card\" + 0.006*\"Windows\" + 0.006*\"use\" + 0.006*\"problem\"\n",
      "Топик: 2 \n",
      "Слова: 0.120*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.022*\"M\" + 0.009*\"Islam\" + 0.009*\"Keith\" + 0.007*\"moral\" + 0.006*\"morality\" + 0.006*\"NNTPPostingHost\" + 0.005*\"Re\" + 0.005*\"motto\" + 0.005*\"Islamic\"\n",
      "\n",
      "Модель с alpha=0.1, eta=0.01:\n",
      "Топик: 0 \n",
      "Слова: 0.023*\"drive\" + 0.010*\"system\" + 0.009*\"writes\" + 0.009*\"article\" + 0.008*\"Re\" + 0.008*\"dont\" + 0.007*\"Subject\" + 0.007*\"Lines\" + 0.007*\"time\" + 0.006*\"disk\"\n",
      "Топик: 1 \n",
      "Слова: 0.021*\"Subject\" + 0.021*\"Lines\" + 0.020*\"Organization\" + 0.013*\"file\" + 0.011*\"University\" + 0.010*\"card\" + 0.010*\"Windows\" + 0.008*\"driver\" + 0.008*\"program\" + 0.008*\"use\"\n",
      "Топик: 2 \n",
      "Слова: 0.082*\"MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX\" + 0.022*\"God\" + 0.014*\"atheist\" + 0.012*\"people\" + 0.012*\"Re\" + 0.011*\"think\" + 0.011*\"say\" + 0.010*\"M\" + 0.010*\"writes\" + 0.009*\"Subject\"\n"
     ]
    }
   ],
   "source": [
    "alpha_values = ['symmetric', 'asymmetric', 0.1]\n",
    "eta_values = ['auto', 0.1, 0.01]\n",
    "\n",
    "# alpha контролирует разнообразие тем в документах\n",
    "# eta влияет на разнообразие слов в темах\n",
    "\n",
    "models = {}\n",
    "dictionary = Dictionary(tokenized_corpus_nav)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_corpus_nav]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for eta in eta_values:\n",
    "        model_key = f\"alpha={alpha}, eta={eta}\"  # tokenized_corpus_nav\n",
    "        models[model_key] = LdaModel(\n",
    "            corpus=bow_corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=3, \n",
    "            random_state=100,\n",
    "            update_every=1,\n",
    "            chunksize=100,\n",
    "            passes=10,\n",
    "            alpha=alpha,\n",
    "            eta=eta\n",
    "        )\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    print(f\"\\nМодель с {model_key}:\")\n",
    "    for idx, topic in model.print_topics(-1):\n",
    "        print(f\"Топик: {idx} \\nСлова: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d7bd89e-fec6-4495-ba94-9b5feaa2c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_repo(tokenized_corpus, num_topics=15):\n",
    "    # Создание словаря\n",
    "    dictionary = Dictionary(tokenized_corpus)\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
    "\n",
    "    lda_model = LdaModel(\n",
    "        corpus=bow_corpus,  \n",
    "        id2word=dictionary,  \n",
    "        num_topics=num_topics,  # Количество тем\n",
    "        passes=10,  # Количество итераций по корпусу\n",
    "        iterations=100,  # Количество итераций для оценки параметров\n",
    "        random_state=42  # Зафиксированный случайный seed для воспроизводимости результатов\n",
    "    )\n",
    "    return lda_model, bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e59d5c9-7ce1-4275-ad5d-ac929742d7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6287da9b2642d0b22c695a112da476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9eeb05484d24136bbd5bdb2b9f170b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b4559a903a4e9c8f5cf502114b5f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45288b9e80344124882654f85560d7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_param = 10 # 1810\n",
    "lda_vectors = []\n",
    "for tokenized_corpus in tokenized_corpuses:\n",
    "    lda_model, bow_corpus = lda_repo(tokenized_corpus, best_param)\n",
    "    # получаем тематические векторы\n",
    "    lda_vec = []\n",
    "    for i in tqdm(range(len(target))):\n",
    "        doc_topics = lda_model.get_document_topics(bow_corpus[i])\n",
    "        doc_vec = np.zeros(best_param)\n",
    "        for topic, prob in doc_topics:\n",
    "            #top_numb = int(re.sub(r'[a-zA-Z]', '', topic))\n",
    "            doc_vec[topic] = prob\n",
    "        lda_vec.append(doc_vec)\n",
    "    lda_vectors.append(lda_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34089f-5300-46ec-8f10-f4eab1345a2f",
   "metadata": {},
   "source": [
    "lsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0d6b0de-7590-4689-8226-99e0cb3b3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_size = 1646  # 1646\n",
    "lsi_represent = []\n",
    "for tokenized_corpus in tf_idf_corpuses:\n",
    "    lsi_model = TruncatedSVD(n_components=mini_size)  # Выбор количества компонентов\n",
    "    lsi_vectors = lsi_model.fit_transform(tokenized_corpus)\n",
    "    lsi_represent.append(lsi_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51326a91-8f20-47e1-8040-21970d011ed0",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7bed6-2621-464c-a69b-b9be45c8b43e",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52adb5b3-c8f6-4b10-b642-4489b7e9d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_gbm(data, target, iter=150, plot_val=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "    train_pool = Pool(X_train, label=y_train)\n",
    "    test_pool = Pool(X_test, label=y_test)\n",
    "    \n",
    "    CatBoost = CatBoostClassifier(iterations=iter, learning_rate=0.1, random_seed=42, loss_function='MultiClass')\n",
    "    CatBoost.fit(train_pool, eval_set=test_pool, verbose=False, plot=plot_val)\n",
    "    y_pred = CatBoost.predict(X_test)\n",
    "    f2_score = fbeta_score(y_test, y_pred, beta=2, average='macro')\n",
    "#    print(\"F2-мера:\", f2_score)\n",
    "    return f2_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fef3c-1b86-4ab1-91d1-640efd91ff0b",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e53366-8551-46ce-b6be-292ce6d9ce49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8eecae80fc40798c10c983f5ec4e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8499536997611281"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(lsi_represent[0], target, iter=150, plot_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "162acba4-97a7-409b-81b2-8ed4e6ceae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8252944313847022"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(lsi_represent[0], target, iter=50)  # noun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd646e-5513-466b-96d1-29380b4171c2",
   "metadata": {},
   "source": [
    "tokenized_corpuses = [tokenized_corpus_n, tokenized_corpus_na, tokenized_corpus_nav, tokenized_corpus_nin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df026154-654e-4a69-af25-1271fc3944a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189387186793669"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(lsi_represent[1], target, iter=50)  # noun adject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "461d2446-948f-4918-8805-f8a8f2f57e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395561051791793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(lsi_represent[2], target, iter=50)  # noun adj verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a618a21b-f37e-4ae9-97ec-6d90470bb6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8123624930928786"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(lsi_represent[3], target, iter=50)  # noun proposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becf145-dddf-424a-9ec6-6e41a9e4b390",
   "metadata": {},
   "source": [
    "### Lda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f57634b-2b9e-4f03-8919-5d1f713dabe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18fff178f784f68872b5c26580c2341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6479268669281357"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(lda_vectors[0], target, iter=150, plot_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "186efa53-4d6d-43f1-8405-8eea7304a6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6431253372332889,\n",
       " 0.6085763079998462,\n",
       " 0.6615513917228233,\n",
       " 0.5313534262164537]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [learning_gbm(lda_vectors[i], target, iter=50) for i in range(4)]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f40289-0734-4ae2-a9d5-0de595adab48",
   "metadata": {},
   "source": [
    "### A bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38068f1d-6215-4887-a7d3-7d0c055665bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a27f7690d04788a02991bf1b0ed8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8386888912581807"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(count_words_corpuses[0], target, iter=150, plot_val=True)  # noun proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59410c8c-3065-424d-a112-75901e8e882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [learning_gbm(count_words_corpuses[i], target, iter=50) for i in range(4)]  # noun proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b726e841-75ed-44bd-8c09-f825ffeafa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7535797600100154,\n",
       " 0.7752726268841157,\n",
       " 0.7625387150061051,\n",
       " 0.7697270000631541]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80d922-0008-49b2-a1e9-dcf007d8595c",
   "metadata": {},
   "source": [
    "### freq_words_corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f30eb2a1-d332-46ad-a03c-02c3a84f3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_words_corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6146225b-5eae-4c7d-80c0-d760f30f9bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0f1b8cbba44f5aba25146347a6da7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8340160774896315"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_gbm(freq_words_corpuses[0], target, iter=150, plot_val=True)  # noun proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b81ea0be-4770-47e0-9563-d56995a30ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [learning_gbm(freq_words_corpuses[i], target, iter=50) for i in range(4)]  # noun proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1e2b5cb-fbbe-46af-ac66-8241f213b91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7486128834086969,\n",
       " 0.7718663462184916,\n",
       " 0.7744106709841607,\n",
       " 0.7687470379309826]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a4d41-bcbd-4a12-8257-02b7570bb3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24b018-93cb-4f51-8767-2b008de225c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c4c17-8093-4422-ba08-99f5daa724be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1050049-b697-4cf3-8e87-cabb0bc9c75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
